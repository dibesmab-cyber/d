# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w1QMEV_cLniM8waiyoQeZVuzex2HUstP
"""

!pip install pandas matplotlib seaborn

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import re
import string
import nltk
from nltk.corpus import stopwords

df = pd.read_csv("/content/500veri.csv")

print("Tekrarlayan satÄ±r sayÄ±sÄ±", df.duplicated().sum())

# veriyle ilgili temel incelemeler

print("ğŸ”¹ Veri Boyutu:", df.shape)
print("\nğŸ”¹ Ä°lk 5 SatÄ±r:")
print(df.head()) #direkt df.head(5) de olur genel yapÄ±yÄ± gÃ¶rmek/kontrol etmek iÃ§in

print("\nğŸ”¹ Veri HakkÄ±nda Bilgi:")
print(df.info())

print("\nğŸ”¹ Eksik DeÄŸerler:")
print(df.isnull().sum())

print("\nğŸ”¹ Temel Ä°statistikler:")
print(df.describe()) #sayÄ±sal sutunlar icin
df.describe(include='object') #kategorik sutunlar icin

for col in df.columns:
    print(f"{col}: {df[col].nunique()} benzersiz deÄŸer")

numeric_cols = [col for col in df.columns if df[col].dtype == 'int']
for col in numeric_cols:
    print(f"\n{col} daÄŸÄ±lÄ±mÄ±:")
    print(df[col].value_counts().head(10))



df.hist(figsize=(3,2), bins=10)
plt.show()

# Numerik kolonlar

skewness = df[numeric_cols].skew()
kurtosis = df[numeric_cols].kurtosis()

#sonuclar
skew_kurt_df = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis})
print("ğŸ“Š Skewness ve Kurtosis:")
print(skew_kurt_df)

#skewness Ã§arpÄ±klÄ±k 0a yakÄ±n normal, pozitif saÄŸ Ã§arpÄ±k, negatif sol Ã§arpÄ±k
#kurtosis basÄ±klÄ±k 0 normale yakÄ±n, >0 sivri, <0 basÄ±k daÄŸÄ±lÄ±m

for col in numeric_cols:
    plt.figure(figsize=(3,2))
    sns.histplot(df[col], kde=True, color="pink", bins=10)  # kernel density estimate
    plt.title(f"{col} - Histogram & KDE")
    plt.xlabel(col)
    plt.ylabel("Frekans")
    plt.show()

nltk.download('stopwords')

turkish_stopwords = set(stopwords.words('turkish'))

def clean_text(text):
    if pd.isnull(text):
        return ""
    text = text.lower()

    text = re.sub(r'[^\w\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼]', ' ', text) #Ã¶zel karakterleri kaldÄ±r
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = ' '.join([word for word in text.split() if word not in turkish_stopwords])
    text = re.sub(r'\s+', ' ', text).strip()

    return text
df = pd.read_csv("/content/500veri.csv")
df["Temiz_Yorum"] = df["Yorum"].apply(clean_text)

df.to_csv("veriler_temiz.csv", index=False)

print("âœ… TemizlenmiÅŸ yorumlar 'veriler_temiz.csv' dosyasÄ±na kaydedildi!")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

df = pd.read_csv("/content/veriler_temiz.csv")

print(df.head())
print(df.columns)

df = pd.read_csv("/content/veriler_temiz.csv")

df = df[['Temiz_Yorum', 'Duygu']]
X = df['Temiz_Yorum']
y = df['Duygu']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)


model = LogisticRegression(max_iter=1000)
model.fit(X_train_vec, y_train)


y_pred = model.predict(X_test_vec)

print("SÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("KarmaÅŸÄ±klÄ±k Matrisi:\n")
cm=confusion_matrix(y_test, y_pred)
print(cm)

labels = ["Negatif", "Notr", "Pozitif"]

plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Tahmin Edilen SÄ±nÄ±f")
plt.ylabel("GerÃ§ek SÄ±nÄ±f")
plt.show()

df = pd.read_csv("/content/veriler_temiz.csv")

df = df[['Yorum', 'Duygu']] # buraya temiz yorum sutunu gelecek


X_train, X_test, y_train, y_test = train_test_split(
    df['Yorum'], df['Duygu'], test_size=0.2, random_state=42
)


vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)


model = LogisticRegression(max_iter=1000)
model.fit(X_train_vec, y_train)


y_pred = model.predict(X_test_vec)


print("SÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("KarmaÅŸÄ±klÄ±k Matrisi:\n")
cm=confusion_matrix(y_test, y_pred)
print(cm)

labels = ["Negatif", "Notr", "Pozitif"]

plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Tahmin Edilen SÄ±nÄ±f")
plt.ylabel("GerÃ§ek SÄ±nÄ±f")
plt.show()

#NÃ–TR Ä°Ã‡Ä°N AUGMENTATION DENEDÄ°M
!pip install nlpaug transformers sentencepiece pandas
import pandas as pd
import nlpaug.augmenter.word as naw


df = pd.read_csv("/content/veriler_temiz.csv")


neutral_df = df[df['Duygu'] == 2]
neutral_texts = neutral_df['Yorum'].tolist()

aug = naw.ContextualWordEmbsAug(
    model_path='bert-base-multilingual-cased',
    action="substitute"
)


augmented_texts = []
for text in neutral_texts:
    augmented = aug.augment(text, n=3)
    augmented_texts.extend(augmented)

# augmented veriyidf olarak hazÄ±rla
aug_df = pd.DataFrame({
    "Duygu": [2]*len(augmented_texts),
    "Yorum Yapan": ["Augmented"]*len(augmented_texts), #yeni eklenen yorum yapan kÄ±smÄ±na augmented etiketi yazÄ±lxcak
    "Yorum": augmented_texts
})


new_df = pd.concat([df, aug_df], ignore_index=True) #birleÅŸtirmr


new_df.to_csv("dataset_augmented.csv", index=False, encoding="utf-8-sig")

print("Augmentation tamamlandÄ± dosya 'dataset_augmented.csv' olarak kaydedildi")
print(f"Temel nÃ¶tr cÃ¼mle sayÄ±sÄ±: {len(neutral_texts)}")
print(f"Augmented nÃ¶tr cÃ¼mle sayÄ±sÄ±: {len(augmented_texts)}")

#augmented verileri temizledim.
nltk.download('stopwords')

turkish_stopwords = set(stopwords.words('turkish'))

def clean_text(text):
    if pd.isnull(text):
        return ""

    text = text.lower()
    text = re.sub(r'[^\w\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼]', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = ' '.join([word for word in text.split() if word not in turkish_stopwords])

    text = re.sub(r'\s+', ' ', text).strip()

    return text

df = pd.read_csv("/content/dataset_augmented.csv")

df["Temiz_Yorum"] = df["Yorum"].apply(clean_text)

df.to_csv("augmented500.csv", index=False)

print("âœ… TemizlenmiÅŸ yorumlar 'augmented500' dosyasÄ±na kaydedildi!")

#AUGMENTED veri iÃ§in sÄ±nÄ±flandÄ±rma sonucu


df = pd.read_csv("/content/augmented500.csv")

df = df[['Yorum', 'Duygu']]


X_train, X_test, y_train, y_test = train_test_split(
    df['Yorum'], df['Duygu'], test_size=0.2, random_state=42
)

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_vec, y_train)

y_pred = model.predict(X_test_vec)

print("SÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("KarmaÅŸÄ±klÄ±k Matrisi:\n")
print(confusion_matrix(y_test, y_pred))

df = pd.read_csv("/content/veriler_temiz.csv")

# pozitif negatif sÃ¶zlÃ¼k
pozitif_kelimeler = [
    "harika", "mÃ¼kemmel", "razÄ± olsun", "teÅŸekkÃ¼rler", "saÄŸlÄ±k", "beÄŸendim", "teÅŸekkÃ¼r", "sayenizde", "mutlu", "sevdim","helal olsun","bravo"
]
negatif_kelimeler = [
    "yazÄ±k", "berbat", "rezalet", "iÄŸrenÃ§", "kapatÄ±l", "rÃ¼ÅŸvet", "iftira", "fena", "Ã§Ã¼rÃ¼me", "kapatÄ±lmalÄ±", "istifa", "bÄ±yÄ±k","soruÅŸturma","yeter","suÃ§"
]


def kelime_bazli_duygu(yorum):
    pozitif_sayi = sum(1 for kelime in pozitif_kelimeler if kelime in yorum)
    negatif_sayi = sum(1 for kelime in negatif_kelimeler if kelime in yorum)

    if pozitif_sayi > negatif_sayi:
        return "Pozitif"
    elif negatif_sayi > pozitif_sayi:
        return "Negatif"
    else:
        return "NÃ¶tr"

#her yorum icin fonksiyonu cagÄ±r
df["Tahmin"] = df["Temiz_Yorum"].apply(kelime_bazli_duygu)


print("Kelime bazlÄ± duygu analizi daÄŸÄ±lÄ±mÄ±:")
print(df["Tahmin"].value_counts())


plt.figure(figsize=(3,3))
df["Tahmin"].value_counts().plot.pie(autopct="%1.1f%%", colors=sns.color_palette("Set2"))
plt.title("Kelime BazlÄ± Duygu Analizi DaÄŸÄ±lÄ±mÄ±")
plt.ylabel("")
plt.show()


print("\nÃ–rnek tahminler:")
print(df[["Temiz_Yorum", "Tahmin"]].head(15))

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score
results = []
df = pd.read_csv("/content/veriler_temiz.csv")
print(df.head())


X = df["Yorum"]
y = df["Duygu"]   # 1Negatif 2NÃ¶tr 3Pozitif


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

models = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": LinearSVC()
}

for name, model in models.items():
    print(f"\nğŸ¤– {name} ğŸ¤–")
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)

    acc = accuracy_score(y_test, y_pred)
    print("Accuracy:", acc)
    print(classification_report(y_test, y_pred, digits=2))

    report = classification_report(y_test, y_pred, output_dict=True)
    results.append({
        "Model": name,
        "Accuracy": acc,
        "F1_Negatif": report["1"]["f1-score"],
        "F1_NÃ¶tr": report["2"]["f1-score"],
        "F1_Pozitif": report["3"]["f1-score"],
    })


results_df = pd.DataFrame(results)
print("\nSonuÃ§ Tablosu:")
print(results_df)
#acc gÃ¶rselleÅŸtrme
plt.figure(figsize=(4,3))
sns.barplot(x="Model", y="Accuracy", data=results_df)
plt.title("Model Accuracy KarÅŸÄ±laÅŸtÄ±rmasÄ±")
plt.ylim(0,1)
plt.show()

#f1 sonucu gÃ¶rselleÅŸirme
f1_df = results_df.melt(id_vars="Model",
                        value_vars=["F1_Negatif", "F1_NÃ¶tr", "F1_Pozitif"],
                        var_name="Class", value_name="F1-Score")

plt.figure(figsize=(5,3))
sns.barplot(x="Model", y="F1-Score", hue="Class", data=f1_df)
plt.title("Her Model iÃ§in F1 SkorlarÄ±")
plt.ylim(0,1)
plt.legend(title="SÄ±nÄ±f")
plt.show()

!pip install catboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from catboost import CatBoostClassifier

# 1. Veriyi oku
df = pd.read_csv("/content/veriler_temiz.csv")

# 2. Gerekli sÃ¼tunlarÄ± al
df = df[['Temiz_Yorum', 'Duygu']]
X = df['Temiz_Yorum']
y = df['Duygu']

# 3. EÄŸitim / test bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. TF-IDF vektÃ¶rleÅŸtirme
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 5. CatBoost modelini tanÄ±mla
model = CatBoostClassifier(
    iterations=500,        # aÄŸaÃ§ sayÄ±sÄ±
    learning_rate=0.1,     # Ã¶ÄŸrenme hÄ±zÄ±
    depth=6,               # aÄŸaÃ§ derinliÄŸi
    loss_function='MultiClass',
    eval_metric='TotalF1',
    verbose=100,           # her 100 iterasyonda Ã§Ä±ktÄ± ver
    random_seed=42
)

# 6. Modeli eÄŸit
model.fit(X_train_vec, y_train, eval_set=(X_test_vec, y_test))

# 7. Tahmin
y_pred = model.predict(X_test_vec)

# 8. Rapor
print("\nSÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("\nKarmaÅŸÄ±klÄ±k Matrisi:\n")
print(confusion_matrix(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# 1. Veriyi oku
df = pd.read_csv("/content/veriler_temiz.csv")

# 2. Gerekli sÃ¼tunlarÄ± al
df = df[['Temiz_Yorum', 'Duygu']]
X = df['Temiz_Yorum']
y = df['Duygu']

# 3. EÄŸitim / test bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. TF-IDF vektÃ¶rleÅŸtirme
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 5. AdaBoost modelini tanÄ±mla
# ZayÄ±f Ã¶ÄŸrenici olarak kÃ¼Ã§Ã¼k derinlikli karar aÄŸacÄ± kullanÄ±yoruz
base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)

model = AdaBoostClassifier(
    estimator=base_estimator,
    n_estimators=400,       # aÄŸaÃ§ sayÄ±sÄ±
    learning_rate=0.9,      # Ã¶ÄŸrenme hÄ±zÄ±
    random_state=42
)

# 6. Modeli eÄŸit
model.fit(X_train_vec.toarray(), y_train)  # AdaBoost sparse desteklemez â†’ array'e Ã§evirdik

# 7. Tahmin
y_pred = model.predict(X_test_vec.toarray())

# 8. Rapor
print("\nSÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("\nKarmaÅŸÄ±klÄ±k Matrisi:\n")
print(confusion_matrix(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from lightgbm import LGBMClassifier

# 1. Veriyi oku
df = pd.read_csv("/content/veriler_temiz.csv")

# 2. Sadece gerekli sÃ¼tunlarÄ± al
df = df[['Temiz_Yorum', 'Duygu']]
X = df['Temiz_Yorum']
y = df['Duygu']

# 3. EÄŸitim / test bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. TF-IDF vektÃ¶rleÅŸtirme
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 5. LightGBM classifier
model = LGBMClassifier(
    objective='multiclass',
    num_class=len(y.unique()),
    learning_rate=0.1,
    n_estimators=200,
    random_state=42,
    verbose=-1
)

# 6. Modeli eÄŸit
model.fit(X_train_vec, y_train,
          eval_set=[(X_test_vec, y_test)],
          eval_metric='multi_logloss',
          )

# 7. Tahmin
y_pred = model.predict(X_test_vec)

# 8. Rapor
print("\nSÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("\nKarmaÅŸÄ±klÄ±k Matrisi:\n")
print(confusion_matrix(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

# 1. Veriyi oku
df = pd.read_csv("/content/veriler_temiz.csv")

# 2. Gerekli sÃ¼tunlarÄ± al
df = df[['Temiz_Yorum', 'Duygu']]
X = df['Temiz_Yorum']
y = df['Duygu']

# 3. EÄŸitim / test bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. TF-IDF vektÃ¶rleÅŸtirme
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 5. Random Forest classifier
model = RandomForestClassifier(
    n_estimators=300,   # aÄŸaÃ§ sayÄ±sÄ±
    max_depth=None,     # sÄ±nÄ±rsÄ±z derinlik
    random_state=42,
    n_jobs=-1           # paralel Ã§alÄ±ÅŸtÄ±r
)

# 6. Modeli eÄŸit
model.fit(X_train_vec, y_train)

# 7. Tahmin
y_pred = model.predict(X_test_vec)

# 8. Rapor
print("\nSÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("\nKarmaÅŸÄ±klÄ±k Matrisi:\n")
print(confusion_matrix(y_test, y_pred))

!pip install

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout

# 1. Veriyi oku
df = pd.read_csv("/content/veriler_temiz.csv")

# 2. Gerekli sÃ¼tunlarÄ± al
df = df[['Temiz_Yorum', 'Duygu']]
X = df['Temiz_Yorum']
y = df['Duygu']

# 3. Label encoding (Ã§Ã¼nkÃ¼ ANN one-hot veya integer ister)
encoder = LabelEncoder()
y = encoder.fit_transform(y)   # Negatif=0, Notr=1, Pozitif=2 gibi

# 4. EÄŸitim / test bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5. TF-IDF vektÃ¶rleÅŸtirme
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train).toarray()
X_test_vec = vectorizer.transform(X_test).toarray()

# 6. ANN modeli
model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train_vec.shape[1],)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3 sÄ±nÄ±f iÃ§in Ã§Ä±kÄ±ÅŸ
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 7. EÄŸit
history = model.fit(
    X_train_vec, y_train,
    epochs=30,
    batch_size=10,
    validation_data=(X_test_vec, y_test),
    verbose=1
)

# 8. Tahmin
y_pred_probs = model.predict(X_test_vec)
y_pred = y_pred_probs.argmax(axis=1)

# 9. Rapor
print("\nSÄ±nÄ±flandÄ±rma Raporu:\n")
print(classification_report(y_test, y_pred, target_names=["Negatif", "Notr", "Pozitif"]))

print("\nKarmaÅŸÄ±klÄ±k Matrisi:\n")
print(confusion_matrix(y_test, y_pred))

